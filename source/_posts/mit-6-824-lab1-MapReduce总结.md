---
title: mit-6.824 lab1 MapReduce总结
toc: true
cover: 'https://img.paulzzh.tech/touhou/random?10'
date: 2022-10-08 23:34:40
categories: 课程
tags: [课程，分布式]
description: 最近开始重新学习 mit-6.824，目前把Lab1做完了，在这里总结一下；源代码根据 MIT 实验的要求，是一个Private的 repo 没有公开，需要源代码的可以联系我，也可以一起交流～
---

最近开始重新学习 mit-6.824，目前把Lab1做完了，在这里总结一下；

源代码根据 MIT 实验的要求，是一个Private的 repo 没有公开，需要源代码的可以联系我，也可以一起交流～

视频学习地址：

-   https://www.bilibili.com/video/BV1R7411t71W/

<br/>

<!--more-->

# **mit-6.824 lab1 MapReduce总结**

## **前言**

目前我学习的是 2020 MIT 6.824 分布式系统，当然现在有了更新的 2021、2022 的版本，不过大致思想基本上都是一样的；

完整的 Lab 说明在这里：

-   http://nil.csail.mit.edu/6.824/2020/labs/lab-mr.html

在做实验之前，建议先看完 Google 的这两篇论文：

-   https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf
-   https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf

并看完 Lecture 1、2:

-   https://www.bilibili.com/video/BV1R7411t71W/

对 MapReduce 有一定的了解，并且对 Go 有一定的了解再下手去做；

<br/>

## **MapReduce简介**









<br/>

## **环境准备**

Lab1 所需要的环境非常简单，只需要你的电脑安装 Go 即可；

>   **实验要求的环境是 Go 1.13，但是我的环境是 Go 1.18；**
>
>   **但是由于 Go 良好向前兼容的特性，也是可以用的！**

获取代码也非常简单，直接通过 Git Clone 下来就可以了：

```bash
$ git clone git://g.csail.mit.edu/6.824-golabs-2020 6.824
$ cd 6.824
```

Clone 下来的代码默认已经提供了一个简单的顺序执行的 MapReduce 实现在：`src/main/mrsequential.go`；

同时在 `mrapps/` 目录下，也提供了几个 MapReduce 的应用（主要是实现 Map、Reduce 两个函数），后面测试的时候会用到；

可以简单的先做一个测试：

```bash
$ cd src/main
# 构建 MapReduce APP 的动态链接库
$ go build -buildmode=plugin ../mrapps/wc.go

# 执行顺序实现的MapReduce例子
$ rm mr-out*
$ go run mrsequential.go wc.so pg*.txt

# 验证结果
$ more mr-out-0
A 509
ABOUT 2
ACT 8
...
```

看到成功输出了文件 `mr-out-0` 就说明我们的环境是OK的！

在我们开始实验之前，先看一下上面的例子到底做了点啥；

首先，通过 `go build -buildmode=plugin ../mrapps/wc.go` 将 WordCount 应用编译为了动态链接库；

>   **在后面使用的时候，使用内置的 `plugin` 库中的 `Lookup` 函数直接加载了 Map 和 Reduce 函数进行调用；** 
>
>   关于 `plugin` 库：
>
>   -   https://pkg.go.dev/plugin

`mrapps/wc.go` 中的逻辑非常简单：

```go
package main

// a word-count application "plugin" for MapReduce.
// go build -buildmode=plugin wc.go

import "../mr"
import "unicode"
import "strings"
import "strconv"

// The map function is called once for each file of input. The first
// argument is the name of the input file, and the second is the
// file's complete contents. You should ignore the input file name,
// and look only at the contents argument. The return value is a slice
// of key/value pairs.
func Map(filename string, contents string) []mr.KeyValue {
	// function to detect word separators.
	ff := func(r rune) bool { return !unicode.IsLetter(r) }

	// split contents into an array of words.
	words := strings.FieldsFunc(contents, ff)

	kva := []mr.KeyValue{}
	for _, w := range words {
		kv := mr.KeyValue{w, "1"}
		kva = append(kva, kv)
	}
	return kva
}

// The reduce function is called once for each key generated by the
// map tasks, with a list of all the values created for that key by
// any map task.
func Reduce(key string, values []string) string {
	// return the number of occurrences of this word.
	return strconv.Itoa(len(values))
}
```

**就是定义了 Map、Reduce 两个函数，而不必关心后面的任务是如何调度的（分布式 or 顺序执行）；**

随后执行：`go run mrsequential.go wc.so pg*.txt` 指定了 MapReduce 动态连接库，以及提前准备好的测试文件（以 `pg` 开头）；

下面来看看顺序执行的 MapReduce 的实现：

src/main/mrsequential.go

```go
package main

// simple sequential MapReduce.
// go run mrsequential.go wc.so pg*.txt

import "fmt"
import "../mr"
import "plugin"
import "os"
import "log"
import "io/ioutil"
import "sort"

// for sorting by key.
type ByKey []mr.KeyValue

// for sorting by key.
func (a ByKey) Len() int           { return len(a) }
func (a ByKey) Swap(i, j int)      { a[i], a[j] = a[j], a[i] }
func (a ByKey) Less(i, j int) bool { return a[i].Key < a[j].Key }

func main() {
	if len(os.Args) < 3 {
		fmt.Fprintf(os.Stderr, "Usage: mrsequential xxx.so inputfiles...\n")
		os.Exit(1)
	}

	mapf, reducef := loadPlugin(os.Args[1])

	// read each input file,
	// pass it to Map,
	// accumulate the intermediate Map output.
	intermediate := []mr.KeyValue{}
	for _, filename := range os.Args[2:] {
		file, err := os.Open(filename)
		if err != nil {
			log.Fatalf("cannot open %v", filename)
		}
		content, err := ioutil.ReadAll(file)
		if err != nil {
			log.Fatalf("cannot read %v", filename)
		}
		file.Close()
		kva := mapf(filename, string(content))
		intermediate = append(intermediate, kva...)
	}

	// a big difference from real MapReduce is that all the
	// intermediate data is in one place, intermediate[],
	// rather than being partitioned into NxM buckets.

	sort.Sort(ByKey(intermediate))

	oname := "mr-out-0"
	ofile, _ := os.Create(oname)

	// call Reduce on each distinct key in intermediate[],
	// and print the result to mr-out-0.
	i := 0
	for i < len(intermediate) {
		j := i + 1
		for j < len(intermediate) && intermediate[j].Key == intermediate[i].Key {
			j++
		}
		values := []string{}
		for k := i; k < j; k++ {
			values = append(values, intermediate[k].Value)
		}
		output := reducef(intermediate[i].Key, values)

		// this is the correct format for each line of Reduce output.
		fmt.Fprintf(ofile, "%v %v\n", intermediate[i].Key, output)

		i = j
	}

	ofile.Close()
}

// load the application Map and Reduce functions
// from a plugin file, e.g. ../mrapps/wc.so
func loadPlugin(filename string) (func(string, string) []mr.KeyValue, func(string, []string) string) {
	p, err := plugin.Open(filename)
	if err != nil {
		log.Fatalf("cannot load plugin %v", filename)
	}
	xmapf, err := p.Lookup("Map")
	if err != nil {
		log.Fatalf("cannot find Map in %v", filename)
	}
	mapf := xmapf.(func(string, string) []mr.KeyValue)
	xreducef, err := p.Lookup("Reduce")
	if err != nil {
		log.Fatalf("cannot find Reduce in %v", filename)
	}
	reducef := xreducef.(func(string, []string) string)

	return mapf, reducef
}
```

直接从 main 函数开始看：

首先从动态连接库中获取 Map 和 Reduce 函数；

随后，读取每一个输入的文件，并使用 Map 函数处理，并将处理后的结果存入 intermediate 临时数组中；

紧接着，对 intermediate 临时数组在 Key 上排序，这也是 MapReduce 论文中要求的；

最后，由于按照 Key 值排序后的中间数组相同的 Key 值连续排列，因此我们对相同 Key 值调用 Reduce 进行聚合，并输出到文件中；

至此我们的顺序执行 MapReduce 结束；

可以看到，顺序执行的 MapReduce 的实现是非常简单的；下面我们来看我们要做的实验；

<br/>

## **实验内容分析**

根据实验内容，我们要做的是：**实现一个 单机、多进程、并行 版本的 MapReduce；**

实现包括两个独立的部分：单独的 master 和 多个并行的 worker 进程，两者通过 rpc 调用通信；

每个 worker 进程都会向 master 索要任务、读取任务指定的文件、执行任务并最终将结果写入多个输出文件中；

并且，为了模拟在分布式场景下程序的 worker 挂掉的场景，master 需要处理当一段时间内 worker 都没有完成任务（实验指定的是10秒钟），则需要将任务交给其他 worker 去做；

实验已经提供了部分代码： `main/mrmaster.go` 和 `main/mrworker.go`（无需修改）；

只需要将实现写在： `mr/master.go`, `mr/worker.go`、 `mr/rpc.go` 即可；

同时，也提供了在开发的时候进行测试的方法：

首先，启动 master 节点：

```bash
$ go run mrmaster.go pg-*.txt
```

随后在多个新的终端启动多个 worker 节点：

```bash
$ go run mrworker.go wc.so
```

即可测试；

同时，实验提供了测试脚本用于整个实验的测试：`test-mr.sh`；

关于实验的一些规则：

-   在处理 Map 任务时，需要将中间产物 Key 切分为 `nReduce` 个 reduce 任务，这个参数是调用 `MakeMaster()` 函数时指定的！
-   第 X 个 Reduce 任务的产物应当命名为：`mr-out-X`；
-   最终输出文件 `mr-out-X` 应当每行一个输出，并且采用 `"%v %v"` 格式（和 `main/mrsequential.go` 一致）；
-   当 `main/mrmaster.go` 调用  `mr/master.go` 的 `Done` 函数返回 true 时，其认为所有任务都已完成，将会退出；
-   当所有的任务都结束，worker 应当退出；一个简单的实现是：当调用 `call` rpc 与 master 通信失败时，认为任务结束；不过一个另外一个做法是当任务结束后，master 下发一个 `请退出` 的任务，交给 worker 去执行；

一些提示：

-   在开发测试时，当你修改了 `mr/` 目录下的文件，你应该重新使用 `go build -buildmode=plugin ../mrapps/wc.go` 编译；
-   一个合理的中间结果文件的命名为： `mr-X-Y`，X 表示 Map 任务Id、Y 表示 Reduce 任务Id；
-   可以使用  `ihash(key)` 函数来计算由哪个 worker 来执行对应 Key 的 Reduce 任务；
-   master 节点会被并发的访问，不要忘了对共享的数据加锁；
-   有的时候 worker 需要等待，比如：只有在所有的 Map 任务都完成，才能开启 Reduce 任务；一个实现方案是：worker 定期轮询 master 去索要任务的时候来询问；另一个实现方案是由 master 节点来定期轮询是否所有的任务都已经完成；
-   master 节点无法区分一个 worker 节点到底是挂掉了，还是执行了一个任务太长时间；最好的做法是master等待一段时间后，放弃那些执行时间过长（本实验中为10s）的 worker，认为他们已经挂了，并重新分配任务；
-   可以使用 `mrapps/crash.go` 测试节点挂掉后的恢复，他的 Map、Reduce 函数会随机直接退出（模拟 worker 节点在被分配任务后挂掉）；
-   <red>**为了保证没有 worker 节点能够看到由于节点崩溃而写入一半的最终产出文件，在 MapReduce 论文中提出了，先创建临时文件，并且在全部写入完成后，重命名的方法（很赞！）；你也应当使用这种方法；**</font>

通过上面的任务，我们可以总结出：

对于 master 节点要做到：

-   **在启动时根据指定的输入文件及 nReduce 数，生成 Map Task 及 Reduce Task；**
-   **通过 RPC 的方式为 Workers 分配可用的 Task 去处理（由 Worker 调用去获取）；**
-   **校验 Task 的完成情况，当所有 Map Task 完成后，推进到 Reduce 阶段，开始派发 Reduce Task；在所有 Reduce Task 完成后标记作业已完成并退出；**
-   **同时，由于 worker 节点存在挂掉的可能，在 master 校验 Task 完成情况时，需要 追踪已分配的 Task 的运行情况，在 Task 超出 10s 仍未完成时，将该 Task 重新分配给其他 Worker 重试；**

对于 worker 节点：

-   **当空闲时调用 RPC 向 master 获取任务；**
-   **获取到任务后，根据不同任务类型调用 Map、Reduce 并输出中间、最终文件；**

<br/>

一个难点在于：在执行 Reduce 任务时，需要根据指定的 nReduce 数进行分配；

解决方法是：我们可以通过计算 `ihash(key)` 来定义中间产物文件，来记录每个 Reduce 所在的文件；

另一个难点在于：有可能存在两个 worker 同时执行同一个任务的情况，此时要保证只有一个 Worker 能够完成结果数据的最终写出，以免出现冲突导致最终观察到重复或缺失的结果数据；

我们可以通过实验给出的提示：通过临时文件的方法解决；即：

Worker 写出数据时，先写出到临时文件（Write），最终确认没有问题后再将其重命名（Commit）为正式结果文件，区分开了 Write 和 Commit 的过程；

而 Commit 的过程可以是 Master 来执行，也可以是 Worker 来执行：

-   **Master Commit**：Worker 向 Master 汇报 Task 完成，**Master 确认该 Task 是否仍属于该 Worker**，是则进行结果文件 Commit，否则直接忽略；
-   **Worker Commit**：Worker 向 Master 汇报 Task 完成，Master 确认该 Task 是否仍属于该 Worker 并响应 Worker，是则 Worker 进行结果文件 Commit，再向 Master 汇报 Commit 完成；

上面两种方法都是可行的；

我的实现选择了 Master Commit，因为可以少一次 RPC 调用，在实现上会更简单；但缺点是所有 Task 最终 Commit 都由 Master 完成，在极端场景下会让 Master 变成整个 MR 过程的性能瓶颈；

<br/>

## **代码实现**

代码的实现主要分为三个部分，并且实验已经提供好了对应的文件：

-   Master 与 Worker 间的 RPC 通信，对应 mr/rpc.go 文件；
-   Master 调度逻辑，对应 mr/master.go 文件；
-   Worker 计算逻辑，对应 mr/worker.go 文件；

下面我们一个一个来看；

### **RPC通信**







<br/>

### **Master调度**







<br/>

### **Worker计算**







<br/>

## **测试验证**













<br/>

# **附录**

源代码根据 MIT 实验的要求，是一个Private的 repo 没有公开，需要源代码的可以联系我，也可以一起交流～

视频学习地址：

-   https://www.bilibili.com/video/BV1R7411t71W/


<br/>
